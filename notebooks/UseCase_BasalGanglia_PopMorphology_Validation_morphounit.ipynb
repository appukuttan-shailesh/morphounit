{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<strong>Note:</strong> non-HBP members should contact “support@humanbrainproject.eu” for access to the validation tools</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "Only Python 3 compatible!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Population of Striatum FSI cells Morphology Validation Use Case\n",
    "\n",
    "***\n",
    "**Aim: ** The average morphometrics of a population of Fast-Spiking Interneurons (FSI) in Striatum, that were digitally reconstructed, is validated against experimental data. Additional plots are provided to \n",
    "visualize some statistics derived from the morphometrics of the individual cells, e.g. linear regression analysis, histograms and Kernel-Distribution-Estimates (KDE) for single features, and bi-dimensional joint KDEs for pair of uncorrelated features.\n",
    " \n",
    "\n",
    "***\n",
    "**Version:** 1.0 (13/02/2020)\n",
    "***\n",
    "**Contributors:**  Pedro Garcia-Rodriguez (CNRS), Shailesh Appukuttan (CNRS)\n",
    "***\n",
    "**Contact:** [pedro.garcia@cnrs.fr](mailto:pedro.garcia@cnrs.fr), [shailesh.appukuttan@cnrs.fr](mailto:shailesh.appukuttan@cnrs.fr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\" style=\"color:black\">\n",
    "<h2>About This Use Case</h2><br />\n",
    "This test shall take as input a directory containing neuronal morphologies (currently supports `swc` format). The user decides whether to run the validations for all available morphologies, or a subset of these.\n",
    "<br /><br />\n",
    "The validations are carried out using 'NeuroM' (https://github.com/BlueBrain/NeuroM). The validation test evaluates the morphology in two stages:\n",
    "<br /><br />\n",
    "1. **Hard Constraints**<br />\n",
    "\n",
    "   Here we evaluate the integrity of the neuronal reconstruction in order to determine if it is appropriate for further evaluations. The evaluations here can be sub-divided into the following NeuroM features (apps):\n",
    "   \n",
    "   - morph_check\n",
    "   - cut_plane_detection <br /><br />\n",
    "   \n",
    "2. **Soft Constraints** [Currently only available for *Fast Spiking Interneurons*]<br />\n",
    "\n",
    "Neuronal reconstructions that pass the 'Hard Constraints' are evaluated here for their morphometric features. The features are extracted using NeuroM's morph_stats app, wherever possible, either directly or as a combination of multiple features.\n",
    "<br />  \n",
    "Some of the features currently included are soma's diameter and the maximal branch order in the dendrites, besides the number of trunk sections, -X,Y,Z- extents, field's diameter and total path-length of both the axon and the dendrites.\n",
    "<br /><br />\n",
    "The average morphometrics of the population of neurons is then computed. Those mean values are then compared against experimentally obtained data, as determined by the particular validation test being executed.\n",
    "<br /><br />\n",
    "Additional plots are provided to visualize some statistics derived from the morphometrics of the individual cells, e.g. linear regression analysis, histograms and Kernel-Distribution-Estimates (KDE) for single features, and bi-dimensional joint KDEs for pairs of uncorrelated features.\n",
    "<br /><br />\n",
    "**Note**: Currently only Fast-Spiking Interneurons (FSI) can be considered, since observation data is missing for other neuron types.\n",
    " </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Seting up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pkg_resources\n",
    "from pkg_resources import parse_version\n",
    "!pip install -q tornado==4.5.3\n",
    "!pip install -q --upgrade hbp-service-client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>**Note:** If you encounter any errors in the below cell, please try to restart the kernel (Kernel -> Restart & Run All)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "req_packages = [    \n",
    "                    {\"sciunit\"                  : {\"min_version\": \"0.2.1\",  \"install_version\": \"0.2.1\"}},\n",
    "                    {\"morphounit\"               : {\"min_version\": \"1.0.4\",  \"install_version\": \"1.0.4\"}},\n",
    "                    {\"hbp_validation_framework\" : {\"min_version\": \"0.5.19\", \"install_version\": \"0.5.19\"}},\n",
    "                    {\"neurom\"                   : {\"min_version\": \"1.4.10\", \"install_version\": \"1.4.10\"}},\n",
    "                    {\"numpy\"                    : {\"min_version\": \"1.16.2\", \"install_version\": \"1.16.2\"}},    \n",
    "                    {\"Jinja2\"                   : {\"min_version\": \"2.10.3\", \"install_version\": \"2.10.3\"}},\n",
    "                    {\"tornado\"                  : {\"min_version\": \"4.5.3\",  \"install_version\": \"4.5.3\"}}\n",
    "                ]\n",
    "\n",
    "def install_req_packages():\n",
    "    # currently handles installations via PyPI and GitHub\n",
    "    for pkg in req_packages:        \n",
    "        for pkg_name, pkg_vinfo in pkg.items():\n",
    "            print(\"Checking for package: {}\".format(pkg_name))        \n",
    "            try:\n",
    "                pkg_resources.get_distribution(pkg_name)        \n",
    "                current_version = parse_version(pkg_resources.get_distribution(pkg_name).version)\n",
    "                print(\"\\t{}: current version = {}\".format(pkg_name, current_version))\n",
    "                if not pkg_vinfo[\"min_version\"] or current_version < parse_version(pkg_vinfo[\"min_version\"]) or current_version > parse_version(pkg_vinfo[\"install_version\"]):                                                \n",
    "                        print(\"\\tInstalling another version of {}.\".format(pkg_name))\n",
    "                        raise\n",
    "            except:            \n",
    "                if \"github.com\" in pkg_vinfo[\"install_version\"]:\n",
    "                    os.system(\"pip install --quiet --no-cache-dir --force-reinstall git+{}\".format(pkg_vinfo[\"install_version\"]))\n",
    "                else:\n",
    "                    os.system(\"pip install --quiet --no-cache-dir --force-reinstall {}=={}\".format(pkg_name, pkg_vinfo[\"install_version\"]))                                \n",
    "                print(\"\\t{}: installed version = {}\".format(pkg_name, pkg_vinfo[\"install_version\"]))\n",
    "\n",
    "install_req_packages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import urllib\n",
    "import sciunit\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "from pandas.io.json import json_normalize\n",
    "from morphounit.utils import neuroM_loader, NeuroM_MorphStats, NeuroM_MorphStats_pop\n",
    "from hbp_validation_framework import utils, TestLibrary, ModelCatalog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Check if Model Catalog and Validation Framework Apps Exist in Collab\n",
    "If the notebook is run inside a Collab, we check if an instance of the Model Catalog and Validation Framework apps exist in the current Collab. If not, we add an instance of each (this will be reflected in the Collab's navigation panel, possibly on reloading the page).\n",
    "\n",
    "NOTE: **HBP_USERNAME** is an optional parameter when the notebook is being run inside the Collaboratory. The notebook can automatically identify your username in this scenario. This parameter needs to be specified if a user wishes to download the notebook and run it locally. Another potential (less likely) reason for specifying this (even within the Collaboratory) is in dealing with access permissions (wanting to run the test with different credentials).\n",
    "\n",
    "NOTE: Even if this notebook is not run inside a Collab, the following cell needs to be executed. It will identify if environment and manage accordingly. When not run inside a Collab, it will simply setup parameters required for the test, and not attempt to create new apps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your HBP username; not essential if running inside the Collaboratory\n",
    "HBP_USERNAME = \"\"\n",
    "testLibrary = TestLibrary(username=HBP_USERNAME)\n",
    "modelCatalog = ModelCatalog.from_existing(testLibrary)\n",
    "\n",
    "try:\n",
    "    collab_path = get_collab_storage_path()\n",
    "    collab_id = collab_path[1:] # this might fail for very old Collabs which use name instead of Collab ID\n",
    "except:\n",
    "    # not run inside Collaboratory\n",
    "    print(\"\\nPlease enter a Collab ID where you wish to store the results:\")\n",
    "    print(\"E.g.: 8123\")\n",
    "    print(\"Note: you should be a member of this Collab!\")\n",
    "    collab_id = input()\n",
    "    if not isinstance(eval(collab_id), int):\n",
    "        raise ValueError(\"Possibly invalid Collab ID: {}. Numeric input expected!\".format(collab_id))    \n",
    "\n",
    "# check if apps exist; if not then create them\n",
    "MCapp_navID = modelCatalog.exists_in_collab_else_create(collab_id)\n",
    "modelCatalog.set_app_config(collab_id=collab_id, app_id=MCapp_navID, only_if_new=\"True\")\n",
    "VFapp_navID = testLibrary.exists_in_collab_else_create(collab_id)\n",
    "testLibrary.set_app_config(collab_id=collab_id, app_id=VFapp_navID, only_if_new=\"True\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Selection: Specifying model from ModelCatalog\n",
    "The user is given an option to choose from existing compatible models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = modelCatalog.get_model(model_id=\"f72f9e36-bb19-496a-8260-7090760319de\") # 2e196088-b39e-4975-a3bb-6dd6d52ddb3f\n",
    "m2 = modelCatalog.get_model(model_id=\"53934bfe-2400-4ba5-852e-be4628d1998e\") # fee9a5eb-fbf1-4bb8-998c-992e3408fd3c\n",
    "m3 = modelCatalog.get_model(model_id=\"e5b7b6dd-753c-4e09-999a-9c4e1c2e5177\") # 25f0ab7b-33a3-48db-8375-7e31dc385c22\n",
    "list_of_models = [m1, m2, m3]\n",
    "df = pd.DataFrame.from_dict(json_normalize(list_of_models), orient='columns')\n",
    "df = df.reindex(columns=['name', 'id', 'author', 'brain_region', 'species', 'cell_type', 'model_scope', 'abstraction_level', 'description'])\n",
    "df.index += 1 \n",
    "print(\"Available models are listed below:\")\n",
    "df.replace('\\n','', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Enter the # of required model: \")\n",
    "choice = int(input())\n",
    "if choice <= len(list_of_models) and choice > 0:   \n",
    "    model_id = list_of_models[choice-1][\"id\"]\n",
    "    model_name = list_of_models[choice-1][\"name\"]\n",
    "else:\n",
    "    raise ValueError(\"Invalid entry for model choice!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 3.1 Gather Additional Info\n",
    "Currently the test is applicable for three cell types: _D1-type MSN_, _D2-type MSN_ and _FS Interneurons_ <br />\n",
    "The usecase tries to identify the cell type from the input directory. If this isn't possible, the user is prompted to specify the cell type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# will try to determine cell type from model name; if not then ask user\n",
    "# current options: msn_d1, msn_d2, fs\n",
    "cell_type_map = {\"msn_d1\" : \"medium spiny neuron (D1 type)\",\n",
    "                 \"msn_d2\" : \"medium spiny neuron (D2 type)\",\n",
    "                 \"fs\"     : \"fast spiking interneuron\"}\n",
    "\n",
    "if \"msn_d1\" in model_name:\n",
    "    cell_type = \"msn_d1\" \n",
    "elif \"msn_d2\" in model_name:\n",
    "    cell_type = \"msn_d2\"\n",
    "elif \"fs\" in model_name:\n",
    "    cell_type = \"fs\"    \n",
    "else:\n",
    "    print(\"\\nPlease enter the cell_type: \")\n",
    "    options = [\"msn_d1\", \"msn_d2\", \"fs\"]\n",
    "    for i, each in enumerate(options,start=1):\n",
    "        print(\"\\t{}. {}\".format(i,each))\n",
    "    print(\"Enter the # of cell_type: \")\n",
    "    choice = str(input())\n",
    "    cell_type = options[choice-1]\n",
    "print(\"Cell Type = {}\".format(cell_type))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Selection of desired model instance\n",
    "The model has several model instances corresponding to various morphologies. These are listed below. The user needs to select one of these model instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_instances = modelCatalog.list_model_instances(model_id=model_id)\n",
    "df = pd.DataFrame.from_dict(json_normalize(model_instances), orient='columns')\n",
    "df = df.reindex(columns=['version', 'id'])\n",
    "df.index += 1\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The user is asked to specify the model instances that they are interested in validating. The valid range of values are indicated. The user can provide input as follows: <br />\n",
    " * All parameters, specify: `all` <br />\n",
    " * Multiple parameter sets, specify numbers as a list, e.g.: `[1,4,5,8]` <br />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Enter a list of, minimum, two model instance(s) to be validated: 1 - {}\".format(len(model_instances)))\n",
    "print(\"Example inputs: [1,10], [1,4,5,8], all\")\n",
    "instances_entry = input().lower()\n",
    "if instances_entry == \"all\":\n",
    "    instances_list = range(1, len(model_instances)+1)\n",
    "else:    \n",
    "    if isinstance(eval(instances_entry), list) and len(instances_entry)>1:\n",
    "        instances_list = eval(instances_entry)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid entry for parameter set!\")  \n",
    "       \n",
    "valid_instances_list = []\n",
    "for i in instances_list:\n",
    "    if i > 0 and i <= len(model_instances):\n",
    "        valid_instances_list.append(i)\n",
    "    else:\n",
    "        print(\"Invalid entry: {}. Excluded.\".format(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A: Hard Constraints Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.1 Instantiating the model; Running the validation tests\n",
    "Validations for hard constraints are run for each of the selected morphologies. At the end of the test, the user is provided with a textual summary of the _score_ and the path to related output files generated by the test. These and other details can be viewed in the  _Validation Framework_ app (see Collab's Navigation panel; select `Validation Framework`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result_uuids = []\n",
    "\n",
    "for index in valid_instances_list:\n",
    "    model_instance_name = model_instances[index-1][\"version\"]    \n",
    "    model_source = model_instances[index-1][\"source\"]\n",
    "    model_path = os.path.abspath(urllib.request.urlretrieve(model_source, os.path.basename(model_source))[0])\n",
    "    \n",
    "    morph_model = neuroM_loader(model_path=model_path, name=model_instance_name)\n",
    "    morph_model.model_uuid = model_id\n",
    "    morph_model.model_version = model_instances[index-1][\"version\"]\n",
    "       \n",
    "    if cell_type in [\"msn_d1\", \"msn_d2\"]:\n",
    "        test_alias = \"basalg_msn_morph_hardChecks\"\n",
    "    else:\n",
    "        test_alias = \"basalg_fs_morph_hardChecks\"\n",
    "    result_id, score = utils.run_test(username=HBP_USERNAME, model=morph_model, test_alias=test_alias, \n",
    "                                      test_version=\"2.0\", storage_collab_id=collab_id, \n",
    "                                      register_result=True, client_obj=testLibrary)\n",
    "    result_uuids.append(result_id)    \n",
    "\n",
    "print(\"The result(s) can be viewed in the HBP Validation Framework app. Direct link(s):\")\n",
    "for result_uuid in result_uuids:\n",
    "    print(\"https://collab.humanbrainproject.eu/#/collab/{}/nav/{}?state=result.{}\".format(str(collab_id),str(VFapp_navID), result_uuid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.2 Score Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(result_uuids) > 0:\n",
    "    df, excluded_results = utils.generate_score_matrix(result_list=result_uuids, collab_id=collab_id, client_obj=modelCatalog)        \n",
    "    from IPython.core.display import HTML\n",
    "    HTML(\"<style>.rendered_html th {max-width: 120px;}</style>\")\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.3 Generate Report\n",
    "The validation framework can generate an HTML report for all the successfully completed tests. The user is prompted whether such a report should be generated for the current validation results. If asked to generate, the location to the generated HTML is indicated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_path = None\n",
    "if len(result_uuids) > 0:\n",
    "    print(\"\\nDo you wish to generate an HTML report of the executed tests?\")\n",
    "    print(\"Enter: y/n\")\n",
    "    choice = input().lower()\n",
    "    valid_choices = {\"yes\": True, \"y\": True, \"no\": False, \"n\": False}\n",
    "    if valid_choices[choice]:\n",
    "        report_path, valid_uuids = utils.generate_HTML_report(result_list=result_uuids, collab_id=collab_id, client_obj=modelCatalog)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.4 View Report Inside Jupyter Notebook\n",
    "The HTML report created in the above cell is displayed within the Jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if report_path:\n",
    "    rel_report_path = os.path.relpath(report_path)\n",
    "    from IPython.display import IFrame    \n",
    "    display(IFrame(rel_report_path, width=\"100%\", height=1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B: Soft Constraints Validation\n",
    "[Currently only available for Fast Spiking Interneurons]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if cell_type != \"fs\":\n",
    "    print(\"Soft constraints validation not currently available for cell type = {}\".format(cell_type))\n",
    "    result_uuids = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B.1 Instantiating the model; Running the validation tests\n",
    "Validations for soft constraints are run for only the morphologies that _pass_ the first part of the test. The average morphometrics of the population of those neurons is then computed. Those mean values are then compared against experimentally obtained data.\n",
    "\n",
    "\n",
    "Additional plots are provided to visualize some statistics derived from the morphometrics of the indiviual cells, e.g. linear regression analysis, histograms and Kernel-Distribution-Estimates (KDE) for single features, and bi-dimensionl joint KDEs for pair of uncorrelated features.\n",
    "\n",
    "At the end of the test, the user is provided with a textual summary of the _mean score_ value (across the features tested) and the path to related output files generated by the test. These and other details can be viewed in the  _Validation Framework_ app (see Collab's Navigation panel; select `Validation Framework`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Obtain instance_uuid of all above morphologies that passed the hard constraints validations\n",
    "# Only these will be evaluated for the soft constraints\n",
    "passed_morph_inst_uuid = []\n",
    "for uuid in result_uuids:\n",
    "    result = testLibrary.get_result(result_id=uuid)\n",
    "    if result[\"results\"][0][\"passed\"]:\n",
    "        passed_morph_inst_uuid.append(result[\"results\"][0][\"model_version_id\"])              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exception handling of resulting repeated models UUIDs, \n",
    "# due to an existing issue in current KG-version of the Model Catalog    \n",
    "duplicates = []\n",
    "for item in passed_morph_inst_uuid:\n",
    "    if passed_morph_inst_uuid.count(item) > 1:\n",
    "        duplicates.append(item)\n",
    "if len(duplicates)!=0:\n",
    "    print(\"Warning: some UUID duplicates were found in list of passed morphologies\")\n",
    "    print(duplicates)\n",
    "\n",
    "# Checking the amount of the cells left after Section A of the test\n",
    "assert ( len(passed_morph_inst_uuid) > 1 ), \\\n",
    "    \"Not enough number of digital morphologies has passed the Section A of the test. \\\n",
    "    At least 2 passed morphologies are needed to form a population to be tested.\"   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Collecting all above morphologies that passed the hard constraints validations into one directory\n",
    "passed_models_path = os.getcwd()+'/neuroM_morph_softChecks_population'\n",
    "if os.path.exists(passed_models_path):\n",
    "    shutil.rmtree(passed_models_path)\n",
    "os.makedirs(passed_models_path)\n",
    "\n",
    "for uuid in passed_morph_inst_uuid:\n",
    "    model_instance = modelCatalog.get_model_instance(instance_id=uuid)\n",
    "    model_name = model_instance[\"version\"]\n",
    "    model_source = model_instance[\"source\"]\n",
    "    morph_path = os.path.abspath(urllib.request.urlretrieve(model_source, os.path.basename(model_source))[0])\n",
    "    shutil.copy(morph_path, passed_models_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Computing the average morphometrics of the population of morpholgies \n",
    "# that passed the hard constraints validations\n",
    "\n",
    "neuroM_stats_file = 'fs_cells_NeuroM_MorphStats_pred.json'\n",
    "\n",
    "morph_pop_path = passed_models_path\n",
    "model_pop_name = os.path.basename(passed_models_path)  # Cells-dir\n",
    "morph_model = NeuroM_MorphStats_pop(model_name=model_pop_name, morph_path=morph_pop_path, \n",
    "                                    neuroM_pred_file=neuroM_stats_file)\n",
    "\n",
    "# Creating a new model instance on the Model Catalog for the chosen \n",
    "# cells collection that passed the hard-constraints Test\n",
    "model_catalog = ModelCatalog(username=HBP_USERNAME)\n",
    "model_id = '078d19ae-8107-476e-8efe-bf7b0e0898bd'\n",
    "morph_model.model_uuid = model_id          # uuid for the chosen cell collection\n",
    "morph_model.model_version = datetime.now().strftime(\"%Y%m%d-%H%M%S\")+'_'+ model_pop_name # model instance\n",
    "\n",
    "model_catalog.add_model_instance(model_id=morph_model.model_uuid, alias='NeuroM-MorphStats-Pop', \n",
    "                                 version=morph_model.model_version,\n",
    "                                 description=', '.join(passed_morph_inst_uuid))\n",
    "from time import sleep\n",
    "sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the test on the population of morpholgies that passed the hard constraints validations\n",
    "test_alias = 'morph_stats_pop_Test'\n",
    "\n",
    "result_uuids = []\n",
    "\n",
    "print('Testing population directory ----> ', morph_pop_path, '\\n\\n')\n",
    "result_id, score = utils.run_test(username=HBP_USERNAME, \n",
    "                           model=morph_model, test_alias=test_alias, test_version=\"4.0\",\n",
    "                           storage_collab_id=collab_id, register_result=True, \n",
    "                           client_obj=testLibrary)\n",
    "   \n",
    "\n",
    "result_uuids.append(result_id)\n",
    "\n",
    "if len(result_uuids) > 0:   \n",
    "    print(\"The result(s) can be viewed in the HBP Validation Framework app. Direct link(s):\")\n",
    "for result_uuid in result_uuids:\n",
    "    print(\"https://collab.humanbrainproject.eu/#/collab/{}/nav/{}?state=result.{}\".format(str(collab_id),str(VFapp_navID), result_uuid))       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B.2 Score Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if len(result_uuids) > 0:\n",
    "    df, excluded_results = utils.generate_score_matrix(environment=\"integration\", result_list=result_uuids, collab_id=collab_id, client_obj=modelCatalog)        \n",
    "    from IPython.core.display import HTML\n",
    "    HTML(\"<style>.rendered_html th {max-width: 120px;}</style>\")\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B.3 Generate Report\n",
    "The validation framework can generate an HTML report for all the successfully completed tests. The user is prompted whether such a report should be generated for the current validation results. If asked to generate, the location to the generated HTML is indicated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "report_path = None\n",
    "if len(result_uuids) > 0:\n",
    "    print(\"\\nDo you wish to generate an HTML report of the executed tests?\")\n",
    "    print(\"Enter: y/n\")\n",
    "    choice = input().lower()\n",
    "    valid_choices = {\"yes\": True, \"y\": True, \"no\": False, \"n\": False}\n",
    "    if valid_choices[choice]:\n",
    "        report_path, valid_uuids = utils.generate_HTML_report(environment=\"integration\", result_list=result_uuids, collab_id=collab_id, client_obj=modelCatalog)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B.4 View Report Inside Jupyter Notebook\n",
    "The HTML report created in the above cell is displayed within the Jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if report_path:\n",
    "    rel_report_path = os.path.relpath(report_path)\n",
    "    from IPython.display import IFrame    \n",
    "    display(IFrame(rel_report_path, width=\"100%\", height=1000))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
